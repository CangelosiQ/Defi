{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Défi Grosses Data 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Début Commun pour tous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'Annex' from 'C:\\\\Users\\\\Quentin\\\\Documents\\\\INSA\\\\5ème ANNEE GMM\\\\Defi\\\\Annex.py'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from os import listdir\n",
    "import imp\n",
    "import Annex\n",
    "imp.reload(Annex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importe les données concaténées:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:99: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='janvier']='1'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:100: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='février']='2'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:101: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='mars']='3'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:102: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='avril']='4'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:103: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='mai']='5'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:104: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='juin']='6'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='juillet']='7'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:106: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='août']='8'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:107: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='septembre']='9'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:108: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='octobre']='10'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:109: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='novembre']='11'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:110: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='décembre']='12'\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'bool' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-38554d78e7fb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m                                                         \u001b[0mimpute_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'drop'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m                                                         \u001b[0mconvert_month2int\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m                                                         date_method='drop')\n\u001b[0m",
      "\u001b[1;32m~\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py\u001b[0m in \u001b[0;36mget_data_raw\u001b[1;34m(scale, add_dummies, var_dummies, train_test_split, sz_test, impute_method, convert_month2int, date_method)\u001b[0m\n\u001b[0;32m     60\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'tH2_obs'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m## !!! Date?\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtest_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.3\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m11\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mscale\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'bool' object is not callable"
     ]
    }
   ],
   "source": [
    "X_train,X_test,Y_train,Y_test,scaler=Annex.get_data_raw(scale=True, \n",
    "                                                        add_dummies=True,\n",
    "                                                        var_dummies=['insee','ddH10_rose4'],\n",
    "                                                        TrainTestSplit=True,\n",
    "                                                        sz_test=0.3,\n",
    "                                                        impute_method='drop',\n",
    "                                                        convert_month2int=True,\n",
    "                                                        date_method='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=Annex.get_data_raw()\n",
    "N_withNA=df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importe les données séparées par type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meteo_quant, meteo_qual, meteo_date, meteo_y=Annex.get_data_tidied()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplace les variables qualitatives par leur indicatrices. \n",
    "\n",
    "**Attention, ici seuls \"mois\" et \"insee\" sont considérés qualitatives!** (what about the wind? what about the \"ech\"?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:70: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='janvier']='1'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:71: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='février']='2'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:72: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='mars']='3'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:73: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='avril']='4'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:74: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='mai']='5'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:75: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='juin']='6'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:76: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='juillet']='7'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:77: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='août']='8'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:78: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='septembre']='9'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:79: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='octobre']='10'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:80: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='novembre']='11'\n",
      "C:\\Users\\Quentin\\Documents\\INSA\\5ème ANNEE GMM\\Defi\\Annex.py:81: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  df.mois[df.mois=='décembre']='12'\n"
     ]
    }
   ],
   "source": [
    "df=Annex.convert_month_to_int(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df.flir1SOL0=df.flir1SOL0.fillna(0)\n",
    "df.fllat1SOL0=df.fllat1SOL0.fillna(0)\n",
    "df.flsen1SOL0=df.flsen1SOL0.fillna(0)\n",
    "df.flvis1SOL0=df.flvis1SOL0.fillna(0)\n",
    "df.rr1SOL0=df.rr1SOL0.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dummies=pd.get_dummies(df[['insee','ddH10_rose4']])\n",
    "df_full_qtt=pd.concat([df,df_dummies],axis=1)\n",
    "df_full_qtt=df_full_qtt.drop(['insee','ddH10_rose4'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sépare les échantillons d'apprentissage et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_full_qtt.isnull().values.any()\n",
    "df_full_qtt.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nous avons éliminé 26396 données soit 13.95 %\n"
     ]
    }
   ],
   "source": [
    "df_clean=df_full_qtt.dropna(axis=0)\n",
    "N_withoutNA=df_clean.shape[0]\n",
    "print(\"Nous avons éliminé %d données soit %0.2f %s\"%(N_withNA-N_withoutNA,(N_withNA-N_withoutNA)/N_withNA*100,'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((114018, 39), (48866, 39))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y=df_clean['tH2_obs']\n",
    "X=df_clean\n",
    "X=X.drop(['tH2_obs'],axis=1) ## !!! Date?\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=11)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant, faites votre vie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "# L'algorithme ds réseaux de neurones nécessite éventuellement une normalisation \n",
    "# des variables explicatives avec les commandes ci-dessous\n",
    "date_train=X_train['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "date_test=X_test['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "X_train=X_train.drop(['date'],axis=1)\n",
    "X_test=X_test.drop(['date'],axis=1)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "Xnet_train = scaler.transform(X_train)  \n",
    "# Meme transformation sur le test\n",
    "Xnet_test = scaler.transform(X_test)\n",
    "#date_train=np.reshape(date_train,(len(date_train),1))\n",
    "#date_test=np.reshape(date_test,(len(date_test),1))\n",
    "#Xnet_train.shape,date_train.shape,type(X_train),type(date_train)\n",
    "\n",
    "#Xnet_train=np.concatenate((Xnet_train,date_train),axis=1)\n",
    "#Xnet_test=np.concatenate((Xnet_test,date_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid=[{\"hidden_layer_sizes\":list([(50,),(60,),(70,),(80,),(100,),(120,)])}]\n",
    "nnet= GridSearchCV(MLPRegressor(max_iter=500),param_grid,cv=10,n_jobs=-1)\n",
    "nnetOpt=nnet.fit(Xnet_train, Y_train)\n",
    "# paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (1. - nnetOpt.best_score_,nnetOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________________________________________________\n",
      "                   Iteration  1                                             Learning rate:1.00e-03, Momentum:0.84, Best so far:inf\n",
      "__________________________________________________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\sklearn\\neural_network\\multilayer_perceptron.py:1306: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.7]] [ 1.1769133]\n",
      "RMSE 1.17691330233 , S2:  1.01e+00\n",
      "__________________________________________________________________________________________________________________________________________\n",
      "                   Iteration  2                                             Learning rate:1.00e-04, Momentum:0.50, Best so far:1.1769\n",
      "__________________________________________________________________________________________________________________________________________\n",
      "[[ 0.5  0.7]\n",
      " [ 0.   0. ]] [ 1.1769133   1.18387694]\n",
      "RMSE 1.18387693901 , S2:  1.01e+00\n",
      "__________________________________________________________________________________________________________________________________________\n",
      "\n",
      "Last learning rate: 0.0001 \n",
      "Momentum: 0.5\n",
      "Last Result: 1.18387693901\n",
      "__________________________________________________________________________________________________________________________________________\n",
      "\n",
      "[[ 0.5  0.7]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pyGPs # Library for Gaussian Processes\n",
    "import sys\n",
    "sys.path.append(\"./libs\")\n",
    "import presentation as pr\n",
    "## Creation Model pyGPs\n",
    "model=pyGPs.GPR()\n",
    "m=pyGPs.mean.Const(0.4)\n",
    "k=pyGPs.cov.RBF(log_ell=-2.5,log_sigma=0)\n",
    "model.setPrior(mean=m,kernel=k)\n",
    "\n",
    "## Momentum\n",
    "mom=[]\n",
    "momentum=0.7\n",
    "start_m=0.5\n",
    "end_m=0.99\n",
    "\n",
    "## Learning Rate\n",
    "l=[]\n",
    "learning_rate=0.5\n",
    "start_lr=-4#1e-6\n",
    "end_lr=-2#5e-4\n",
    "\n",
    "## Nb Nodes\n",
    "nn=[]\n",
    "nb_nodes=0.5\n",
    "start_nn=20\n",
    "end_nn=200\n",
    "\n",
    "sz_space=100\n",
    "\n",
    "v=np.reshape(np.linspace(0,1,sz_space),(sz_space,1))\n",
    "v_mom=np.reshape(np.linspace(0,1,sz_space),(sz_space,1))\n",
    "X1,X2=np.meshgrid(v,v_mom)\n",
    "V=np.concatenate([np.reshape(X1,(sz_space**2,1)),np.reshape(X2,(sz_space**2,1))],axis=1)\n",
    "v=np.reshape(v,(len(v),))\n",
    "\n",
    "nb_div=0\n",
    "best_so_far=np.Inf\n",
    "RMSE=[]\n",
    "for i in range(10):\n",
    "    l=np.append(l,learning_rate)\n",
    "    mom=np.append(mom,momentum)\n",
    "\n",
    "    HP=np.concatenate([np.reshape(l,(len(l),1)),np.reshape(mom,(len(mom),1))],axis=1)\n",
    "    if i>1 and ((np.sum(np.abs(HP[-1,:]-HP[-2,:]))<1e-5 and np.sum(np.abs(HP[-1,:]))>1e-7) or ys2[ind_min]<1.4e-2):\n",
    "        print('MINIMA MIGHT BE FOUND, quitting iteration',i)\n",
    "        print(np.sum(np.abs(HP[-1,:]-HP[-2,:])),HP[-1,:],HP[-2,:])\n",
    "        print('ys2[ind_min]',ys2[ind_min])\n",
    "        break\n",
    "    pr.big_iter(i+1,'Learning rate:%0.2e, Momentum:%0.2f, Best so far:%0.4f'% (10**(learning_rate*(end_lr-start_lr)+start_lr),momentum*(end_m-start_m)+start_m,best_so_far))\n",
    "\n",
    "    ## Evaluation\n",
    "    alpha=10**(learning_rate*(end_lr-start_lr)+start_lr)\n",
    "    beta=momentum*(end_m-start_m)+start_m\n",
    "    mlp=MLPRegressor(max_iter=500,hidden_layer_sizes =(80,),learning_rate='constant',learning_rate_init=alpha,momentum=beta)\n",
    "    nnetOpt=mlp.fit(Xnet_train, Y_train)\n",
    "    Y_pred = nnetOpt.predict(Xnet_test)\n",
    "    score=mean_squared_error(Y_test,Y_pred)\n",
    "    RMSE=np.append(RMSE,score)\n",
    "    if RMSE[-1]<best_so_far:\n",
    "        best_so_far=RMSE[-1]\n",
    "        best_net=mlp\n",
    "        #best_ind=\n",
    "    \n",
    "    ## Gaussian Processes\n",
    "    print(HP,RMSE)\n",
    "    model.getPosterior(HP,RMSE)\n",
    "    # model.optimize(l,Y)\n",
    "    ym,ys2,fm,fs2,lp=model.predict(V)\n",
    "    # print(ym.shape,V.shape)\n",
    "\n",
    "    ## Updates\n",
    "    ind_min=np.argmin(ym-ys2)\n",
    "    learning_rate=V[ind_min,0]\n",
    "    momentum=V[ind_min,1]\n",
    "    print('RMSE',score,', S2:%10.2e'%ys2[ind_min])\n",
    "    ## Plots\n",
    "    \n",
    "\n",
    "## Final Outputs\n",
    "pr.line()\n",
    "print('Last learning rate:',10**(l[-1]*(end_lr-start_lr)+start_lr),'\\nMomentum:',mom[-1]*(end_m-start_m)+start_m)\n",
    "print('Last Result:',score)\n",
    "\n",
    "pr.line()\n",
    "#print('Best result:\\n',best_ind,best_net,best_so_far)\n",
    "HP=HP[:-1,:]\n",
    "print(HP)\n",
    "#plots('log')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class MLPRegressor in module sklearn.neural_network.multilayer_perceptron:\n",
      "\n",
      "class MLPRegressor(BaseMultilayerPerceptron, sklearn.base.RegressorMixin)\n",
      " |  Multi-layer Perceptron regressor.\n",
      " |  \n",
      " |  This model optimizes the squared-loss using LBFGS or stochastic gradient\n",
      " |  descent.\n",
      " |  \n",
      " |  .. versionadded:: 0.18\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  hidden_layer_sizes : tuple, length = n_layers - 2, default (100,)\n",
      " |      The ith element represents the number of neurons in the ith\n",
      " |      hidden layer.\n",
      " |  \n",
      " |  activation : {'identity', 'logistic', 'tanh', 'relu'}, default 'relu'\n",
      " |      Activation function for the hidden layer.\n",
      " |  \n",
      " |      - 'identity', no-op activation, useful to implement linear bottleneck,\n",
      " |        returns f(x) = x\n",
      " |  \n",
      " |      - 'logistic', the logistic sigmoid function,\n",
      " |        returns f(x) = 1 / (1 + exp(-x)).\n",
      " |  \n",
      " |      - 'tanh', the hyperbolic tan function,\n",
      " |        returns f(x) = tanh(x).\n",
      " |  \n",
      " |      - 'relu', the rectified linear unit function,\n",
      " |        returns f(x) = max(0, x)\n",
      " |  \n",
      " |  solver : {'lbfgs', 'sgd', 'adam'}, default 'adam'\n",
      " |      The solver for weight optimization.\n",
      " |  \n",
      " |      - 'lbfgs' is an optimizer in the family of quasi-Newton methods.\n",
      " |  \n",
      " |      - 'sgd' refers to stochastic gradient descent.\n",
      " |  \n",
      " |      - 'adam' refers to a stochastic gradient-based optimizer proposed by\n",
      " |        Kingma, Diederik, and Jimmy Ba\n",
      " |  \n",
      " |      Note: The default solver 'adam' works pretty well on relatively\n",
      " |      large datasets (with thousands of training samples or more) in terms of\n",
      " |      both training time and validation score.\n",
      " |      For small datasets, however, 'lbfgs' can converge faster and perform\n",
      " |      better.\n",
      " |  \n",
      " |  alpha : float, optional, default 0.0001\n",
      " |      L2 penalty (regularization term) parameter.\n",
      " |  \n",
      " |  batch_size : int, optional, default 'auto'\n",
      " |      Size of minibatches for stochastic optimizers.\n",
      " |      If the solver is 'lbfgs', the classifier will not use minibatch.\n",
      " |      When set to \"auto\", `batch_size=min(200, n_samples)`\n",
      " |  \n",
      " |  learning_rate : {'constant', 'invscaling', 'adaptive'}, default 'constant'\n",
      " |      Learning rate schedule for weight updates.\n",
      " |  \n",
      " |      - 'constant' is a constant learning rate given by\n",
      " |        'learning_rate_init'.\n",
      " |  \n",
      " |      - 'invscaling' gradually decreases the learning rate ``learning_rate_``\n",
      " |        at each time step 't' using an inverse scaling exponent of 'power_t'.\n",
      " |        effective_learning_rate = learning_rate_init / pow(t, power_t)\n",
      " |  \n",
      " |      - 'adaptive' keeps the learning rate constant to\n",
      " |        'learning_rate_init' as long as training loss keeps decreasing.\n",
      " |        Each time two consecutive epochs fail to decrease training loss by at\n",
      " |        least tol, or fail to increase validation score by at least tol if\n",
      " |        'early_stopping' is on, the current learning rate is divided by 5.\n",
      " |  \n",
      " |      Only used when solver='sgd'.\n",
      " |  \n",
      " |  learning_rate_init : double, optional, default 0.001\n",
      " |      The initial learning rate used. It controls the step-size\n",
      " |      in updating the weights. Only used when solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  power_t : double, optional, default 0.5\n",
      " |      The exponent for inverse scaling learning rate.\n",
      " |      It is used in updating effective learning rate when the learning_rate\n",
      " |      is set to 'invscaling'. Only used when solver='sgd'.\n",
      " |  \n",
      " |  max_iter : int, optional, default 200\n",
      " |      Maximum number of iterations. The solver iterates until convergence\n",
      " |      (determined by 'tol') or this number of iterations. For stochastic\n",
      " |      solvers ('sgd', 'adam'), note that this determines the number of epochs\n",
      " |      (how many times each data point will be used), not the number of\n",
      " |      gradient steps.\n",
      " |  \n",
      " |  shuffle : bool, optional, default True\n",
      " |      Whether to shuffle samples in each iteration. Only used when\n",
      " |      solver='sgd' or 'adam'.\n",
      " |  \n",
      " |  random_state : int, RandomState instance or None, optional, default None\n",
      " |      If int, random_state is the seed used by the random number generator;\n",
      " |      If RandomState instance, random_state is the random number generator;\n",
      " |      If None, the random number generator is the RandomState instance used\n",
      " |      by `np.random`.\n",
      " |  \n",
      " |  tol : float, optional, default 1e-4\n",
      " |      Tolerance for the optimization. When the loss or score is not improving\n",
      " |      by at least tol for two consecutive iterations, unless `learning_rate`\n",
      " |      is set to 'adaptive', convergence is considered to be reached and\n",
      " |      training stops.\n",
      " |  \n",
      " |  verbose : bool, optional, default False\n",
      " |      Whether to print progress messages to stdout.\n",
      " |  \n",
      " |  warm_start : bool, optional, default False\n",
      " |      When set to True, reuse the solution of the previous\n",
      " |      call to fit as initialization, otherwise, just erase the\n",
      " |      previous solution.\n",
      " |  \n",
      " |  momentum : float, default 0.9\n",
      " |      Momentum for gradient descent update.  Should be between 0 and 1. Only\n",
      " |      used when solver='sgd'.\n",
      " |  \n",
      " |  nesterovs_momentum : boolean, default True\n",
      " |      Whether to use Nesterov's momentum. Only used when solver='sgd' and\n",
      " |      momentum > 0.\n",
      " |  \n",
      " |  early_stopping : bool, default False\n",
      " |      Whether to use early stopping to terminate training when validation\n",
      " |      score is not improving. If set to true, it will automatically set\n",
      " |      aside 10% of training data as validation and terminate training when\n",
      " |      validation score is not improving by at least tol for two consecutive\n",
      " |      epochs.\n",
      " |      Only effective when solver='sgd' or 'adam'\n",
      " |  \n",
      " |  validation_fraction : float, optional, default 0.1\n",
      " |      The proportion of training data to set aside as validation set for\n",
      " |      early stopping. Must be between 0 and 1.\n",
      " |      Only used if early_stopping is True\n",
      " |  \n",
      " |  beta_1 : float, optional, default 0.9\n",
      " |      Exponential decay rate for estimates of first moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  beta_2 : float, optional, default 0.999\n",
      " |      Exponential decay rate for estimates of second moment vector in adam,\n",
      " |      should be in [0, 1). Only used when solver='adam'\n",
      " |  \n",
      " |  epsilon : float, optional, default 1e-8\n",
      " |      Value for numerical stability in adam. Only used when solver='adam'\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  loss_ : float\n",
      " |      The current loss computed with the loss function.\n",
      " |  \n",
      " |  coefs_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the weight matrix corresponding\n",
      " |      to layer i.\n",
      " |  \n",
      " |  intercepts_ : list, length n_layers - 1\n",
      " |      The ith element in the list represents the bias vector corresponding to\n",
      " |      layer i + 1.\n",
      " |  \n",
      " |  n_iter_ : int,\n",
      " |      The number of iterations the solver has ran.\n",
      " |  \n",
      " |  n_layers_ : int\n",
      " |      Number of layers.\n",
      " |  \n",
      " |  n_outputs_ : int\n",
      " |      Number of outputs.\n",
      " |  \n",
      " |  out_activation_ : string\n",
      " |      Name of the output activation function.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  MLPRegressor trains iteratively since at each time step\n",
      " |  the partial derivatives of the loss function with respect to the model\n",
      " |  parameters are computed to update the parameters.\n",
      " |  \n",
      " |  It can also have a regularization term added to the loss function\n",
      " |  that shrinks model parameters to prevent overfitting.\n",
      " |  \n",
      " |  This implementation works with data represented as dense and sparse numpy\n",
      " |  arrays of floating point values.\n",
      " |  \n",
      " |  References\n",
      " |  ----------\n",
      " |  Hinton, Geoffrey E.\n",
      " |      \"Connectionist learning procedures.\" Artificial intelligence 40.1\n",
      " |      (1989): 185-234.\n",
      " |  \n",
      " |  Glorot, Xavier, and Yoshua Bengio. \"Understanding the difficulty of\n",
      " |      training deep feedforward neural networks.\" International Conference\n",
      " |      on Artificial Intelligence and Statistics. 2010.\n",
      " |  \n",
      " |  He, Kaiming, et al. \"Delving deep into rectifiers: Surpassing human-level\n",
      " |      performance on imagenet classification.\" arXiv preprint\n",
      " |      arXiv:1502.01852 (2015).\n",
      " |  \n",
      " |  Kingma, Diederik, and Jimmy Ba. \"Adam: A method for stochastic\n",
      " |      optimization.\" arXiv preprint arXiv:1412.6980 (2014).\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      MLPRegressor\n",
      " |      BaseMultilayerPerceptron\n",
      " |      abc.NewBase\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, hidden_layer_sizes=(100,), activation='relu', solver='adam', alpha=0.0001, batch_size='auto', learning_rate='constant', learning_rate_init=0.001, power_t=0.5, max_iter=200, shuffle=True, random_state=None, tol=0.0001, verbose=False, warm_start=False, momentum=0.9, nesterovs_momentum=True, early_stopping=False, validation_fraction=0.1, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the multi-layer perceptron model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      y : array-like, shape (n_samples, n_outputs)\n",
      " |          The predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  fit(self, X, y)\n",
      " |      Fit the model to data matrix X and target(s) y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like or sparse matrix, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          The target values (class labels in classification, real numbers in\n",
      " |          regression).\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from BaseMultilayerPerceptron:\n",
      " |  \n",
      " |  partial_fit\n",
      " |      Fit the model to data matrix X and target y.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix}, shape (n_samples, n_features)\n",
      " |          The input data.\n",
      " |      \n",
      " |      y : array-like, shape (n_samples,)\n",
      " |          The target values.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns a trained MLP model.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : boolean, optional\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Returns the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like, shape = (n_samples, n_features)\n",
      " |          Test samples.\n",
      " |      \n",
      " |      y : array-like, shape = (n_samples) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like, shape = [n_samples], optional\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(MLPRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp=MLPRegressor(max_iter=500,hidden_layer_sizes =(50,))\n",
    "nnetOpt=mlp.fit(Xnet_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimation de l'erreur de prévision sur le test\n",
    "1-nnetOpt.score(Xnet_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "# prévision de l'échantillon test\n",
    "Y_pred = nnetOpt.predict(Xnet_test)\n",
    "print(\"MSE =\",mean_squared_error(Y_test,Y_pred))\n",
    "print(\"R2 =\",r2_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "capeinsSOL0        float64\n",
       "ciwcH20            float64\n",
       "clwcH20            float64\n",
       "ffH10              float64\n",
       "flir1SOL0          float64\n",
       "fllat1SOL0         float64\n",
       "flsen1SOL0         float64\n",
       "flvis1SOL0         float64\n",
       "hcoulimSOL0        float64\n",
       "huH2               float64\n",
       "iwcSOL0            float64\n",
       "nbSOL0_HMoy        float64\n",
       "nH20               float64\n",
       "ntSOL0_HMoy        float64\n",
       "pMER0              float64\n",
       "rr1SOL0            float64\n",
       "rrH20              float64\n",
       "tH2                float64\n",
       "tH2_VGrad_2.100    float64\n",
       "tH2_XGrad          float64\n",
       "tH2_YGrad          float64\n",
       "tpwHPA850          float64\n",
       "ux1H10             float64\n",
       "vapcSOL0           float64\n",
       "vx1H10             float64\n",
       "ech                  int64\n",
       "mois                 int32\n",
       "insee_6088001        uint8\n",
       "insee_31069001       uint8\n",
       "insee_33281001       uint8\n",
       "insee_35281001       uint8\n",
       "insee_59343001       uint8\n",
       "insee_67124001       uint8\n",
       "insee_75114001       uint8\n",
       "ddH10_rose4_1.0      uint8\n",
       "ddH10_rose4_2.0      uint8\n",
       "ddH10_rose4_3.0      uint8\n",
       "ddH10_rose4_4.0      uint8\n",
       "dtype: object"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================== Test n° 1  alpha= 0.35  mom= 0.9  wd= 0  struct= [38 15  1]\n",
      "N,mini_batch_size 114018 11401.800000000001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "./libs\\NNet.py:58: RuntimeWarning: overflow encountered in exp\n",
      "  print(W.shape,X.shape,b.shape)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ERROR! Theta element 426, with value 1.379028e-02, has finite difference gradient 3.198023e+01 but analytic gradient 6.431872e+01. Difference: 32.3384866699  ( 50.2784992665 %) Ratio: 2.01120236768\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2870: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "Y_train=Y_train.reshape((len(Y_train),1))\n",
    "Y_test=Y_test.reshape((len(Y_test),1))\n",
    "import sys\n",
    "sys.path.append(\"./libs\")\n",
    "import NNet\n",
    "import imp\n",
    "imp.reload(NNet)\n",
    "n_output=[Y_train.shape[1]]\n",
    "n_hid=[7]\n",
    "n_input=[X_train.shape[1]]\n",
    "\n",
    "type_layer=['logistic','linear']\n",
    "threshold=1e-4\n",
    "max_iter=1000\n",
    "mini_batch_coef=0.1\n",
    "do_early_stopping=False\n",
    "\n",
    "min_training_loss=np.Inf\n",
    "min_val_loss=np.Inf\n",
    "min_test_loss=np.Inf\n",
    "\n",
    "i=0\n",
    "for alpha in [0.35]:#[0.002,0.01,0.05,0.2,1,5]:\n",
    "    for mom in [0.9]:\n",
    "        for wd in [0]:#[0.001,0,1,0.1,10,0.0001]:\n",
    "            for n_hid in [[15]]:#[30,20],,[40,30],[10,15],[20,20]\n",
    "                i=i+1\n",
    "                sizes=np.concatenate([n_input,n_hid,n_output])\n",
    "                print('==================== Test n°',i,' alpha=',alpha,' mom=',mom,' wd=',wd,' struct=',sizes)\n",
    "                [model,losses]=NNet.build_model(X_train.T, Y_train.T, wd, sizes, type_layer, 'least_square',\n",
    "                                                   max_iter, alpha, mom,\n",
    "                                                   do_early_stopping, mini_batch_coef, threshold,\n",
    "                                                   print_info=True,train_test_split=False,\n",
    "                                                   X_test=X_test.T,y_test=Y_test.T)\n",
    "                if losses[0]<min_training_loss:\n",
    "                    min_training_loss=losses[0]\n",
    "                    min_alpha=alpha\n",
    "                    momentum=mom\n",
    "                    wd_t=wd\n",
    "                    n_hid_t=n_hid\n",
    "                if losses[1]<min_val_loss:\n",
    "                    min_val_loss=losses[1]\n",
    "                    min_alpha_val=alpha\n",
    "                    momentum_val=mom\n",
    "                    wd_v=wd\n",
    "                    n_hid_v=n_hid\n",
    "                if losses[2]<min_test_loss:\n",
    "                    min_test_loss=losses[2]\n",
    "                    min_alpha_test=alpha\n",
    "                    momentum_test=mom\n",
    "                    wd_test=wd\n",
    "                    n_hid_test=n_hid\n",
    "                    # best_class_perf=class_perf[2]\n",
    "\n",
    "print(i, ' configurations have been tested' )\n",
    "\n",
    "print('==== Training test')\n",
    "print('Min alpha: ',min_alpha)\n",
    "print('Momentum: ',momentum)\n",
    "print('Min val loss: ',min_training_loss)\n",
    "print('WD: ',wd_t)\n",
    "print('n_hid: ',n_hid_t)\n",
    "\n",
    "print('==== Validation test')\n",
    "print('Min alpha: ',min_alpha_val)\n",
    "print('Momentum: ',momentum_val)\n",
    "print('Min val loss: ',min_val_loss)\n",
    "print('WD: ',wd_v)\n",
    "print('n_hid: ',n_hid_v)\n",
    "\n",
    "print('==== Generalisation test')\n",
    "print('Min alpha: ',min_alpha_test)\n",
    "print('Momentum: ',momentum_test)\n",
    "print('Min val loss: ',min_test_loss)\n",
    "print('WD: ',wd_test)\n",
    "print('n_hid: ',n_hid_test)\n",
    "# print('Classification performance: ',best_class_perf)\n",
    "\n",
    "print('\\nElapsed time:',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-6-45539d121c0b>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-6-45539d121c0b>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    np.sum(is.na(df_test))\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "df_test=Annex.load_test_set()\n",
    "np.sum(is.na(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp.reload(Annex)\n",
    "df_TEST=Annex.load_test_set()\n",
    "#df_TEST.shape, df_TEST.info()\n",
    "df_TEST=Annex.convert_month_to_int(df_TEST)\n",
    "df_dummies=pd.get_dummies(df_TEST[['insee']])\n",
    "df_TEST_full_qtt=pd.concat([df_TEST,df_dummies],axis=1)\n",
    "df_TEST_full_qtt.flir1SOL0=df_TEST_full_qtt.flir1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.fllat1SOL0=df_TEST_full_qtt.fllat1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.flsen1SOL0=df_TEST_full_qtt.flsen1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.flvis1SOL0=df_TEST_full_qtt.flvis1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.rr1SOL0=df_TEST_full_qtt.rr1SOL0.fillna(0)\n",
    "df_TEST_full_qtt=df_TEST_full_qtt.drop(['insee','date'],axis=1)\n",
    "X_TEST = scaler.transform(df_TEST_full_qtt)  \n",
    "Y_PRED = nnetOpt.predict(X_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Annex.generate_submission_file('submission_17nov2017_10h30.csv', Y_PRED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
