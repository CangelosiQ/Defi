{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Défi Grosses Data 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Début Commun pour tous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from os import listdir\n",
    "import Annex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importe les données concaténées:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=Annex.get_data_raw()\n",
    "N_withNA=df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importe les données séparées par type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meteo_quant, meteo_qual, meteo_date, meteo_y=Annex.get_data_tidied()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplace les variables qualitatives par leur indicatrices. \n",
    "\n",
    "**Attention, ici seuls \"mois\" et \"insee\" sont considérés qualitatives!** (what about the wind? what about the \"ech\"?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=Annex.convert_month_to_int(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.flir1SOL0=df.flir1SOL0.fillna(0)\n",
    "df.fllat1SOL0=df.fllat1SOL0.fillna(0)\n",
    "df.flsen1SOL0=df.flsen1SOL0.fillna(0)\n",
    "df.flvis1SOL0=df.flvis1SOL0.fillna(0)\n",
    "df.rr1SOL0=df.rr1SOL0.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dummies=pd.get_dummies(df[['insee']])\n",
    "df_full_qtt=pd.concat([df,df_dummies],axis=1)\n",
    "df_full_qtt=df_full_qtt.drop(['insee'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sépare les échantillons d'apprentissage et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_full_qtt.isnull().values.any()\n",
    "df_full_qtt.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean=df_full_qtt.dropna(axis=0)\n",
    "N_withoutNA=df_clean.shape[0]\n",
    "print(\"Nous avons éliminé %d données soit %0.2f %s\"%(N_withNA-N_withoutNA,(N_withNA-N_withoutNA)/N_withNA*100,'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y=df_clean['tH2_obs']\n",
    "X=df_clean\n",
    "X=X.drop(['tH2_obs'],axis=1) ## !!! Date?\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=11)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant, faites votre vie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler  \n",
    "# L'algorithme ds réseaux de neurones nécessite éventuellement une normalisation \n",
    "# des variables explicatives avec les commandes ci-dessous\n",
    "date_train=X_train['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "date_test=X_test['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "X_train=X_train.drop(['date'],axis=1)\n",
    "X_test=X_test.drop(['date'],axis=1)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "Xnet_train = scaler.transform(X_train)  \n",
    "# Meme transformation sur le test\n",
    "Xnet_test = scaler.transform(X_test)\n",
    "#date_train=np.reshape(date_train,(len(date_train),1))\n",
    "#date_test=np.reshape(date_test,(len(date_test),1))\n",
    "#Xnet_train.shape,date_train.shape,type(X_train),type(date_train)\n",
    "\n",
    "#Xnet_train=np.concatenate((Xnet_train,date_train),axis=1)\n",
    "#Xnet_test=np.concatenate((Xnet_test,date_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid=[{\"hidden_layer_sizes\":list([(50,),(60,),(70,),(80,)])}]\n",
    "nnet= GridSearchCV(MLPRegressor(max_iter=500),param_grid,cv=10,n_jobs=-1)\n",
    "nnetOpt=nnet.fit(Xnet_train, Y_train)\n",
    "# paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (1. - nnetOpt.best_score_,nnetOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp=MLPRegressor(max_iter=500,hidden_layer_sizes =(50,))\n",
    "nnetOpt=mlp.fit(Xnet_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimation de l'erreur de prévision sur le test\n",
    "1-nnetOpt.score(Xnet_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "# prévision de l'échantillon test\n",
    "Y_pred = nnetOpt.predict(Xnet_test)\n",
    "print(\"MSE =\",mean_squared_error(Y_test,Y_pred))\n",
    "print(\"R2 =\",r2_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train=Y_train.reshape((len(Y_train),1))\n",
    "Y_test=Y_test.reshape((len(Y_test),1))\n",
    "import sys\n",
    "sys.path.append(\"./libs\")\n",
    "#import generic_evolutionary_algorithm as gea\n",
    "import NNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import NNet\n",
    "import imp\n",
    "imp.reload(NNet)\n",
    "n_output=[Y_train.shape[1]]\n",
    "n_hid=[7]\n",
    "n_input=[X_train.shape[1]]\n",
    "\n",
    "type_layer=['logistic','logistic','linear']\n",
    "threshold=1e-4\n",
    "max_iter=1000\n",
    "mini_batch_coef=0.1\n",
    "do_early_stopping=False\n",
    "\n",
    "min_training_loss=np.Inf\n",
    "min_val_loss=np.Inf\n",
    "min_test_loss=np.Inf\n",
    "\n",
    "i=0\n",
    "for alpha in [0.35]:#[0.002,0.01,0.05,0.2,1,5]:\n",
    "    for mom in [0.9]:\n",
    "        for wd in [0]:#[0.001,0,1,0.1,10,0.0001]:\n",
    "            for n_hid in [[15,15]]:#[30,20],,[40,30],[10,15],[20,20]\n",
    "                i=i+1\n",
    "                print('==================== Test n°',i,' alpha=',alpha,' mom=',mom,' wd=',wd,' n_hid=',n_hid)\n",
    "                sizes=np.concatenate([n_input,n_hid,n_output])\n",
    "                [model,losses]=NNet.build_model(X_train.T, Y_train, wd, sizes, type_layer, 'least_square',\n",
    "                                                   max_iter, alpha, mom,\n",
    "                                                   do_early_stopping, mini_batch_coef, threshold,\n",
    "                                                   print_info=True,train_test_split=False,\n",
    "                                                   X_test=X_test.T,y_test=Y_test)\n",
    "                if losses[0]<min_training_loss:\n",
    "                    min_training_loss=losses[0]\n",
    "                    min_alpha=alpha\n",
    "                    momentum=mom\n",
    "                    wd_t=wd\n",
    "                    n_hid_t=n_hid\n",
    "                if losses[1]<min_val_loss:\n",
    "                    min_val_loss=losses[1]\n",
    "                    min_alpha_val=alpha\n",
    "                    momentum_val=mom\n",
    "                    wd_v=wd\n",
    "                    n_hid_v=n_hid\n",
    "                if losses[2]<min_test_loss:\n",
    "                    min_test_loss=losses[2]\n",
    "                    min_alpha_test=alpha\n",
    "                    momentum_test=mom\n",
    "                    wd_test=wd\n",
    "                    n_hid_test=n_hid\n",
    "                    # best_class_perf=class_perf[2]\n",
    "\n",
    "print(i, ' configurations have been tested' )\n",
    "\n",
    "print('==== Training test')\n",
    "print('Min alpha: ',min_alpha)\n",
    "print('Momentum: ',momentum)\n",
    "print('Min val loss: ',min_training_loss)\n",
    "print('WD: ',wd_t)\n",
    "print('n_hid: ',n_hid_t)\n",
    "\n",
    "print('==== Validation test')\n",
    "print('Min alpha: ',min_alpha_val)\n",
    "print('Momentum: ',momentum_val)\n",
    "print('Min val loss: ',min_val_loss)\n",
    "print('WD: ',wd_v)\n",
    "print('n_hid: ',n_hid_v)\n",
    "\n",
    "print('==== Generalisation test')\n",
    "print('Min alpha: ',min_alpha_test)\n",
    "print('Momentum: ',momentum_test)\n",
    "print('Min val loss: ',min_test_loss)\n",
    "print('WD: ',wd_test)\n",
    "print('n_hid: ',n_hid_test)\n",
    "# print('Classification performance: ',best_class_perf)\n",
    "\n",
    "print('\\nElapsed time:',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "E = gea.Evolution(50,\n",
    "      args_pop=(['logistic'],  # activation function for hidden\n",
    "                               # layers except the last one\n",
    "                               # (which is linear positive)\n",
    "                12,  # maximum number of nodes per layer\n",
    "                3,    # maximum number of layers\n",
    "                X_train.shape[1],     # number of inputs\n",
    "                Y_train.shape[1],    # number of outputs\n",
    "                X_train,        # inputs data\n",
    "                Y_train,       # targets data\n",
    "                gea.default_loss_func,\n",
    "                gea.evaluation_NN_hybrid\n",
    "                )\n",
    "      )\n",
    "\n",
    "trialErrors = E.execute(nb_changes=[10],    # int: number of changes to occur\n",
    "     # for evolution / list: iterations in which changes\n",
    "     # will occur (see doc)\n",
    "     doublons=False,     # wheter to keep doublons or not\n",
    "     # (see doc)\n",
    "     similar=False,      # wheter to keep similar neural\n",
    "     # nets or not (see doc)\n",
    "     loss_threshold=1e-3,\n",
    "     echo_level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp.reload(Annex)\n",
    "df_TEST=Annex.load_test_set()\n",
    "#df_TEST.shape, df_TEST.info()\n",
    "df_TEST=Annex.convert_month_to_int(df_TEST)\n",
    "df_dummies=pd.get_dummies(df_TEST[['insee']])\n",
    "df_TEST_full_qtt=pd.concat([df_TEST,df_dummies],axis=1)\n",
    "df_TEST_full_qtt.flir1SOL0=df_TEST_full_qtt.flir1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.fllat1SOL0=df_TEST_full_qtt.fllat1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.flsen1SOL0=df_TEST_full_qtt.flsen1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.flvis1SOL0=df_TEST_full_qtt.flvis1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.rr1SOL0=df_TEST_full_qtt.rr1SOL0.fillna(0)\n",
    "df_TEST_full_qtt=df_TEST_full_qtt.drop(['insee','date'],axis=1)\n",
    "X_TEST = scaler.transform(df_TEST_full_qtt)  \n",
    "Y_PRED = nnetOpt.predict(X_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Annex.generate_submission_file('submission_17nov2017_10h30.csv', Y_PRED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
