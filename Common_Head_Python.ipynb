{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Défi Grosses Data 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Début Commun pour tous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "from os import listdir\n",
    "import imp\n",
    "import Annex\n",
    "imp.reload(Annex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importe les données concaténées:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,Y_train,Y_test,scaler=Annex.get_data_raw(scale=True, \n",
    "                                                        add_dummies=True,\n",
    "                                                        var_dummies=['insee','ddH10_rose4'],\n",
    "                                                        TrainTestSplit=True,\n",
    "                                                        sz_test=0.3,\n",
    "                                                        impute_method='drop',\n",
    "                                                        convert_month2int=True,\n",
    "                                                        date_method='drop')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=Annex.get_data_raw()\n",
    "N_withNA=df.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importe les données séparées par type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meteo_quant, meteo_qual, meteo_date, meteo_y=Annex.get_data_tidied()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remplace les variables qualitatives par leur indicatrices. \n",
    "\n",
    "**Attention, ici seuls \"mois\" et \"insee\" sont considérés qualitatives!** (what about the wind? what about the \"ech\"?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=Annex.convert_month_to_int(df)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df.flir1SOL0=df.flir1SOL0.fillna(0)\n",
    "df.fllat1SOL0=df.fllat1SOL0.fillna(0)\n",
    "df.flsen1SOL0=df.flsen1SOL0.fillna(0)\n",
    "df.flvis1SOL0=df.flvis1SOL0.fillna(0)\n",
    "df.rr1SOL0=df.rr1SOL0.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_dummies=pd.get_dummies(df[['insee','ddH10_rose4']])\n",
    "df_full_qtt=pd.concat([df,df_dummies],axis=1)\n",
    "df_full_qtt=df_full_qtt.drop(['insee','ddH10_rose4'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sépare les échantillons d'apprentissage et de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#df_full_qtt.isnull().values.any()\n",
    "df_full_qtt.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_clean=df_full_qtt.dropna(axis=0)\n",
    "N_withoutNA=df_clean.shape[0]\n",
    "print(\"Nous avons éliminé %d données soit %0.2f %s\"%(N_withNA-N_withoutNA,(N_withNA-N_withoutNA)/N_withNA*100,'%'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Y=df_clean['tH2_obs']\n",
    "X=df_clean\n",
    "X=X.drop(['tH2_obs'],axis=1) ## !!! Date?\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.3,random_state=11)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maintenant, faites votre vie!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler  \n",
    "# L'algorithme ds réseaux de neurones nécessite éventuellement une normalisation \n",
    "# des variables explicatives avec les commandes ci-dessous\n",
    "date_train=X_train['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "date_test=X_test['date'].apply(lambda x: x.strftime('%Y-%m-%d'))\n",
    "X_train=X_train.drop(['date'],axis=1)\n",
    "X_test=X_test.drop(['date'],axis=1)\n",
    "\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(X_train)  \n",
    "Xnet_train = scaler.transform(X_train)  \n",
    "# Meme transformation sur le test\n",
    "Xnet_test = scaler.transform(X_test)\n",
    "#date_train=np.reshape(date_train,(len(date_train),1))\n",
    "#date_test=np.reshape(date_test,(len(date_test),1))\n",
    "#Xnet_train.shape,date_train.shape,type(X_train),type(date_train)\n",
    "\n",
    "#Xnet_train=np.concatenate((Xnet_train,date_train),axis=1)\n",
    "#Xnet_test=np.concatenate((Xnet_test,date_test),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid=[{\"hidden_layer_sizes\":list([(100,)])}]#(50,),(60,),(70,),(80,),,(120,)\n",
    "nnet= GridSearchCV(MLPRegressor(max_iter=500),param_grid,cv=10,n_jobs=-1)\n",
    "nnetOpt=nnet.fit(X_train, Y_train)\n",
    "# paramètre optimal\n",
    "print(\"Meilleur score = %f, Meilleur paramètre = %s\" % (1. - nnetOpt.best_score_,nnetOpt.best_params_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "import pyGPs # Library for Gaussian Processes\n",
    "import sys\n",
    "sys.path.append(\"./libs\")\n",
    "import presentation as pr\n",
    "## Creation Model pyGPs\n",
    "model=pyGPs.GPR()\n",
    "m=pyGPs.mean.Const(0.4)\n",
    "k=pyGPs.cov.RBF(log_ell=-2.5,log_sigma=0)\n",
    "model.setPrior(mean=m,kernel=k)\n",
    "\n",
    "## Momentum\n",
    "mom=[]\n",
    "momentum=0.7\n",
    "start_m=0.5\n",
    "end_m=0.99\n",
    "\n",
    "## Learning Rate\n",
    "l=[]\n",
    "learning_rate=0.5\n",
    "start_lr=-4#1e-6\n",
    "end_lr=-2#5e-4\n",
    "\n",
    "## Nb Nodes\n",
    "nn=[]\n",
    "nb_nodes=0.5\n",
    "start_nn=20\n",
    "end_nn=200\n",
    "\n",
    "sz_space=100\n",
    "\n",
    "v=np.reshape(np.linspace(0,1,sz_space),(sz_space,1))\n",
    "v_mom=np.reshape(np.linspace(0,1,sz_space),(sz_space,1))\n",
    "X1,X2=np.meshgrid(v,v_mom)\n",
    "V=np.concatenate([np.reshape(X1,(sz_space**2,1)),np.reshape(X2,(sz_space**2,1))],axis=1)\n",
    "v=np.reshape(v,(len(v),))\n",
    "\n",
    "nb_div=0\n",
    "best_so_far=np.Inf\n",
    "RMSE=[]\n",
    "for i in range(10):\n",
    "    l=np.append(l,learning_rate)\n",
    "    mom=np.append(mom,momentum)\n",
    "\n",
    "    HP=np.concatenate([np.reshape(l,(len(l),1)),np.reshape(mom,(len(mom),1))],axis=1)\n",
    "    if i>1 and ((np.sum(np.abs(HP[-1,:]-HP[-2,:]))<1e-5 and np.sum(np.abs(HP[-1,:]))>1e-7) or ys2[ind_min]<1.4e-2):\n",
    "        print('MINIMA MIGHT BE FOUND, quitting iteration',i)\n",
    "        print(np.sum(np.abs(HP[-1,:]-HP[-2,:])),HP[-1,:],HP[-2,:])\n",
    "        print('ys2[ind_min]',ys2[ind_min])\n",
    "        break\n",
    "    pr.big_iter(i+1,'Learning rate:%0.2e, Momentum:%0.2f, Best so far:%0.4f'% (10**(learning_rate*(end_lr-start_lr)+start_lr),momentum*(end_m-start_m)+start_m,best_so_far))\n",
    "\n",
    "    ## Evaluation\n",
    "    alpha=10**(learning_rate*(end_lr-start_lr)+start_lr)\n",
    "    beta=momentum*(end_m-start_m)+start_m\n",
    "    mlp=MLPRegressor(max_iter=500,hidden_layer_sizes =(80,),learning_rate='constant',learning_rate_init=alpha,momentum=beta)\n",
    "    nnetOpt=mlp.fit(Xnet_train, Y_train)\n",
    "    Y_pred = nnetOpt.predict(Xnet_test)\n",
    "    score=mean_squared_error(Y_test,Y_pred)\n",
    "    RMSE=np.append(RMSE,score)\n",
    "    if RMSE[-1]<best_so_far:\n",
    "        best_so_far=RMSE[-1]\n",
    "        best_net=mlp\n",
    "        #best_ind=\n",
    "    \n",
    "    ## Gaussian Processes\n",
    "    print(HP,RMSE)\n",
    "    model.getPosterior(HP,RMSE)\n",
    "    # model.optimize(l,Y)\n",
    "    ym,ys2,fm,fs2,lp=model.predict(V)\n",
    "    # print(ym.shape,V.shape)\n",
    "\n",
    "    ## Updates\n",
    "    ind_min=np.argmin(ym-ys2)\n",
    "    learning_rate=V[ind_min,0]\n",
    "    momentum=V[ind_min,1]\n",
    "    print('RMSE',score,', S2:%10.2e'%ys2[ind_min])\n",
    "    ## Plots\n",
    "    \n",
    "\n",
    "## Final Outputs\n",
    "pr.line()\n",
    "print('Last learning rate:',10**(l[-1]*(end_lr-start_lr)+start_lr),'\\nMomentum:',mom[-1]*(end_m-start_m)+start_m)\n",
    "print('Last Result:',score)\n",
    "\n",
    "pr.line()\n",
    "#print('Best result:\\n',best_ind,best_net,best_so_far)\n",
    "HP=HP[:-1,:]\n",
    "print(HP)\n",
    "#plots('log')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "help(MLPRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mlp=MLPRegressor(max_iter=500,hidden_layer_sizes =(50,))\n",
    "nnetOpt=mlp.fit(Xnet_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Estimation de l'erreur de prévision sur le test\n",
    "1-nnetOpt.score(X_test,Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "# prévision de l'échantillon test\n",
    "Y_pred = nnetOpt.predict(Xnet_test)\n",
    "print(\"MSE =\",mean_squared_error(Y_test,Y_pred))\n",
    "print(\"R2 =\",r2_score(Y_test,Y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train=Y_train.reshape((len(Y_train),1))\n",
    "Y_test=Y_test.reshape((len(Y_test),1))\n",
    "import sys\n",
    "sys.path.append(\"./libs\")\n",
    "import NNet\n",
    "import imp\n",
    "imp.reload(NNet)\n",
    "n_output=[Y_train.shape[1]]\n",
    "n_hid=[7]\n",
    "n_input=[X_train.shape[1]]\n",
    "\n",
    "type_layer=['logistic','linear']\n",
    "threshold=1e-4\n",
    "max_iter=1000\n",
    "mini_batch_coef=0.1\n",
    "do_early_stopping=False\n",
    "\n",
    "min_training_loss=np.Inf\n",
    "min_val_loss=np.Inf\n",
    "min_test_loss=np.Inf\n",
    "\n",
    "i=0\n",
    "for alpha in [0.35]:#[0.002,0.01,0.05,0.2,1,5]:\n",
    "    for mom in [0.9]:\n",
    "        for wd in [0]:#[0.001,0,1,0.1,10,0.0001]:\n",
    "            for n_hid in [[15]]:#[30,20],,[40,30],[10,15],[20,20]\n",
    "                i=i+1\n",
    "                sizes=np.concatenate([n_input,n_hid,n_output])\n",
    "                print('==================== Test n°',i,' alpha=',alpha,' mom=',mom,' wd=',wd,' struct=',sizes)\n",
    "                [model,losses]=NNet.build_model(X_train.T, Y_train.T, wd, sizes, type_layer, 'least_square',\n",
    "                                                   max_iter, alpha, mom,\n",
    "                                                   do_early_stopping, mini_batch_coef, threshold,\n",
    "                                                   print_info=True,train_test_split=False,\n",
    "                                                   X_test=X_test.T,y_test=Y_test.T)\n",
    "                if losses[0]<min_training_loss:\n",
    "                    min_training_loss=losses[0]\n",
    "                    min_alpha=alpha\n",
    "                    momentum=mom\n",
    "                    wd_t=wd\n",
    "                    n_hid_t=n_hid\n",
    "                if losses[1]<min_val_loss:\n",
    "                    min_val_loss=losses[1]\n",
    "                    min_alpha_val=alpha\n",
    "                    momentum_val=mom\n",
    "                    wd_v=wd\n",
    "                    n_hid_v=n_hid\n",
    "                if losses[2]<min_test_loss:\n",
    "                    min_test_loss=losses[2]\n",
    "                    min_alpha_test=alpha\n",
    "                    momentum_test=mom\n",
    "                    wd_test=wd\n",
    "                    n_hid_test=n_hid\n",
    "                    # best_class_perf=class_perf[2]\n",
    "\n",
    "print(i, ' configurations have been tested' )\n",
    "\n",
    "print('==== Training test')\n",
    "print('Min alpha: ',min_alpha)\n",
    "print('Momentum: ',momentum)\n",
    "print('Min val loss: ',min_training_loss)\n",
    "print('WD: ',wd_t)\n",
    "print('n_hid: ',n_hid_t)\n",
    "\n",
    "print('==== Validation test')\n",
    "print('Min alpha: ',min_alpha_val)\n",
    "print('Momentum: ',momentum_val)\n",
    "print('Min val loss: ',min_val_loss)\n",
    "print('WD: ',wd_v)\n",
    "print('n_hid: ',n_hid_v)\n",
    "\n",
    "print('==== Generalisation test')\n",
    "print('Min alpha: ',min_alpha_test)\n",
    "print('Momentum: ',momentum_test)\n",
    "print('Min val loss: ',min_test_loss)\n",
    "print('WD: ',wd_test)\n",
    "print('n_hid: ',n_hid_test)\n",
    "# print('Classification performance: ',best_class_perf)\n",
    "\n",
    "print('\\nElapsed time:',time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test=Annex.load_test_set()\n",
    "np.sum(is.na(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imp.reload(Annex)\n",
    "df_TEST=Annex.load_test_set()\n",
    "#df_TEST.shape, df_TEST.info()\n",
    "df_TEST=Annex.convert_month_to_int(df_TEST)\n",
    "df_dummies=pd.get_dummies(df_TEST[['insee']])\n",
    "df_TEST_full_qtt=pd.concat([df_TEST,df_dummies],axis=1)\n",
    "df_TEST_full_qtt.flir1SOL0=df_TEST_full_qtt.flir1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.fllat1SOL0=df_TEST_full_qtt.fllat1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.flsen1SOL0=df_TEST_full_qtt.flsen1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.flvis1SOL0=df_TEST_full_qtt.flvis1SOL0.fillna(0)\n",
    "df_TEST_full_qtt.rr1SOL0=df_TEST_full_qtt.rr1SOL0.fillna(0)\n",
    "df_TEST_full_qtt=df_TEST_full_qtt.drop(['insee','date'],axis=1)\n",
    "X_TEST = scaler.transform(df_TEST_full_qtt)  \n",
    "Y_PRED = nnetOpt.predict(X_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Annex.generate_submission_file('submission_17nov2017_10h30.csv', Y_PRED)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
